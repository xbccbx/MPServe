# MPServe 
This repository contains related GPU kernel implementation of our paper "Redundancy-Reduced Storage and Fast Decompression for Multi-Precision LLM Serving". More compression, inference, and testing scripts will be added in a few days.

## Scripts
To compile the environment, run the following script:
```
python setup.py build_ext --inplace
```
Compress a specific model: TODO
Test Throughput of our kernel: TODO
Inference on an MPServe model: TODO
Test in application scenarios: TODO.

All scripts will be updated before February 13, 2026.